---
title: "Backprop"
categories:
  - Blog
tags:
  - ML
toc: true
toc_sticky: true
---

# Design matrix

Let's start with the design matrix $\mathbf{X}$ definition

$$
\mathbf{X}=\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
$$

This is a $m \times k$ matrix when we have $k$ features and $m$ training examples. For example, maybe we have $k=3$ features such as a student's GPA, GRE and class rank and we have $m=1000$ students in our sample, then we'd have a $1000 \times 3$ design matrix. Each row of that matrix is a vector representing one of our training examples, and the superscript here denotes which training example we are considering.

# Forward prop


## First hidden layer

<img src="/assets/images/nn.png" alt="Simple NN" class="full">


Consider if we had the next layer with just 2 nodes then the linear transformation would look like

### Linear step

$$
\mathbf{Z}^{[1]} = \mathbf{X}\mathbf{W}^{[1]}+\mathbf{b}^{[1]}
$$

where square-bracket superscripts denote the layer under consideration

$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
\begin{pmatrix}
    \vert & \vert \\
    w_1^{[1]}   & w_2^{[1]}   \\
    \vert & \vert
\end{pmatrix}
+
\quad
\begin{pmatrix}
b^{[1]}\\
\vdots\\
b^{[1]}
\end{pmatrix}
$$

so we had $\mathbf{X}$ being an $m \times k$ matrix, and $\mathbf{W}^{[1]}$ - the matrix of weights for the first layer - is a $k \times n_1$ dimensional matrix if we have $n_1$ nodes in this hidden layer. Here $n_1=2$. If have $m$ training examples we need $m \times 1$ bias units too, so $\mathbf{b}$ is a $m \times 1$ column vector.


Imagine the above with a single training example or $m=1$

$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](1)} & \hspace{-0.2cm} \text{---} 
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} 
\end{pmatrix}
\begin{pmatrix}
    \vert & \vert \\
    w_1^{[1]}   & w_2^{[1]}   \\
    \vert & \vert
\end{pmatrix}
+
\quad
b^{[1]}

$$


We can see that $\mathbf{z}^{[1](1)}$ is just a row-vector of shape $1\times n_1$, with each element representing the (linear) output of the neurons in the 1st hidden layer.

E.g. 

$$\mathbf{Z}^{[1]}_{11}$$

would mean the output from the first node, in the first hidden layer after being fed the first training example. 

and 

$$\mathbf{Z}^{[1]}_{12}$$

would be the output from the second node in the first hidden layer, after being fed the first training example. 

Note: $\mathbf{Z}=\mathbf{X}\mathbf{W}$ could also be written as $\mathbf{Z}^T=\mathbf{W}^T\mathbf{X}^T$
and some people may prefer that convention.


### Non-linear step and activation


Next, we would apply an activation function, such as the sigmoid, tanh or ReLu to the outputs from the first hidden layer.

$$
\mathbf{A^{[1]}} = \sigma(\mathbf{Z}^{[1]})
$$

where $\mathbf{A}^{[1]}$ will also be a $m \times n_1$ dimensional matrix.


## Subsequent hidden layers

All that happens now is a repeat, instead of feeding the next layer with the design matrix, $\mathbf{X}$, we now feed it $\mathbf{A}^{[1]}$


### Linear step


$$
\mathbf{Z}^{[2]} = \mathbf{A^{[1]}}\mathbf{W}^{[2]}+\mathbf{b}^{[2]}
$$