---
title: "Backprop"
categories:
  - Blog
tags:
  - ML
toc: true
toc_sticky: true
---

# Design matrix

Let's start with the design matrix $\mathbf{X}$ definition

$$
\mathbf{X}=\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
$$

This is a $m \times k$ matrix when we have $k$ features and $m$ training examples. For example, maybe we have $k=3$ features such as a student's GPA, GRE and class rank and we have $m=1000$ students in our sample, then we'd have a $1000 \times 3$ design matrix. Each row of that matrix is a vector representing one of our training examples, and the superscript here denotes which training example we are considering.

# Forward prop

<img src="/assets/images/nn.png" alt="Simple NN" class="full">


Consider if we had the next layer with just 2 nodes then the linear transformation would look like

$$
\mathbf{Z}^{[1]} = \mathbf{X}\mathbf{W}^{[1]}+\mathbf{b}^{[1]}
$$

where square-bracket superscripts denote the layer under consideration

$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
\begin{pmatrix}
    \vert & \vert \\
    w_1^{[1]}   & w_2^{[1]}   \\
    \vert & \vert
\end{pmatrix}
+
\quad
\begin{pmatrix}
b^{[1]}\\
\vdots\\
b^{[1]}
\end{pmatrix}
$$

so we had $\mathbf{X}$ being an $m \times k$ matrix, and $\mathbf{W}^{[1]}$ - the matrix of weights for the first layer - is a $k \times n_1$ dimensional matrix if we have $n_1$ nodes in this hidden layer. Here $n_1=2$. If have $m$ training examples we need $m \times 1$ bias units too, so $\mathbf{b}$ is a $m \times 1$ column vector.


Imagine the above with a single training example or $m=1$

$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{(1)} & \hspace{-0.2cm} \text{---} 
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} 
\end{pmatrix}
\begin{pmatrix}
    \vert & \vert \\
    w_1^{[1]}   & w_2^{[1]}   \\
    \vert & \vert
\end{pmatrix}
+
\quad
b^{[1]}

$$


We can see that $\mathbf{z}^{[1]}$ is just a row-vector of shape $1\times n_1$, with each element representing the (linear) output of the neurons in the 1st hidden layer.

E.g. $\mathbf{Z}^{[1]}_{11}$ would mean the output from the first node, in the first hidden layer after being fed the first training example. $\mathbf{Z}^{[1]}_{12}$ would be the output from the second node in the first hidden layer, after being fed the first training example. 

