---
title: "Backprop"
categories:
  - Blog
tags:
  - ML
toc: true
toc_sticky: true
---

# Design matrix

Let's start with the design matrix $\mathbf{X}$ definition

$$
\mathbf{X}=\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
$$

This is a $m \times k$ matrix when we have $k$ features and $m$ training examples. For example, maybe we have $k=3$ features such as a student's GPA, GRE and class rank and we have $m=1000$ students in our sample, then we'd have a $1000 \times 3$ design matrix. Each row of that matrix is a vector representing one of our training examples, and the superscript here denotes which training example we are considering.

# Forward prop


## First hidden layer

<img src="/assets/images/nn.png" alt="Simple NN" class="full">


Consider if we had the next layer with just 2 nodes then the linear transformation would look like

### Linear step

$$
\mathbf{Z}^{[1]} = \mathbf{X}\mathbf{W}^{[1]}+\mathbf{b}^{[1]}
$$

where square-bracket superscripts denote the layer under consideration

$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
\begin{pmatrix}
    \vert & \vert \\
    w_1^{[1]}   & w_2^{[1]}   \\
    \vert & \vert
\end{pmatrix}
+
\quad
\begin{pmatrix}
b^{[1]}\\
\vdots\\
b^{[1]}
\end{pmatrix}
$$

so we had $\mathbf{X}$ being an $m \times k$ matrix, and $\mathbf{W}^{[1]}$ - the matrix of weights for the first layer - is a $k \times n_1$ dimensional matrix if we have $n_1$ nodes in this hidden layer. Here $n_1=2$. If have $m$ training examples we need $m \times 1$ bias units too, so $\mathbf{b}$ is a $m \times 1$ column vector.


Imagine the above with a single training example or $m=1$

$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](1)} & \hspace{-0.2cm} \text{---} 
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} 
\end{pmatrix}
\begin{pmatrix}
    \vert & \vert \\
    w_1^{[1]}   & w_2^{[1]}   \\
    \vert & \vert
\end{pmatrix}
+
\quad
b^{[1]}

$$


We can see that 

$$\mathbf{z}^{[1](1)}$$

 is just a row-vector of shape $1\times n_1$, with each element representing the (linear) output of the neurons in the 1st hidden layer when applied to the first training example.

E.g. 

$$\mathbf{Z}^{[1]}_{11}$$

would mean the output from the first node, in the first hidden layer after being fed the first training example. 

and 

$$\mathbf{Z}^{[1]}_{12}$$

would be the output from the second node in the first hidden layer, after being fed the first training example. 

Going even further, if we reduced the nodes in this layer to just $1$, then we'd have the Perceptron,
and the output for a single training example would just be a scalar.

Note: $\mathbf{Z}=\mathbf{X}\mathbf{W}$ could also be written as $\mathbf{Z}^T=\mathbf{W}^T\mathbf{X}^T$
and some people may prefer that convention.




### Non-linear step and activation


Next, we would apply an activation function, such as the sigmoid, tanh or ReLu to the outputs from the first hidden layer.

$$
\mathbf{A^{[1]}} = \sigma(\mathbf{Z}^{[1]})
$$

where $\mathbf{A}^{[1]}$ will also be a $m \times n_1$ dimensional matrix.


## Subsequent hidden layers

All that happens now is a repeat, instead of feeding the next layer with the design matrix, $\mathbf{X}$, we now feed it $\mathbf{A}^{[1]}$


### Linear step


$$
\mathbf{Z}^{[2]} = \mathbf{A^{[1]}}\mathbf{W}^{[2]}+\mathbf{b}^{[2]}
$$

Remember $\mathbf{A}^{[1]}$ is a $m \times n_1$ dimensional matrix, where $m$ is the number of training examples we have and $n_1$ is how many nodes are in the first hidden layer.  

This is a bit like we now have $m$ training examples with $n_1$ features each. Each node of the 2nd hidden layer, needs to accept input from each of those $n_1$ features (or outputs from the previous layer if you prefer), meaning each node of the 2nd layer will need $n_1$ weights.

If we have $n_2$ nodes in the 2nd hidden layer, then $\mathbf{W}^{[2]}$ needs to be a $n_1 \times n_2$ dimensional matrix


$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[2](1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[2](m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{a}^{[1](1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{a}^{[1](m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
\begin{pmatrix}
    \vert & \dots & \vert \\
    w_1^{[2]} & \dots  & w_{n_2}^{[2]}   \\
    \vert & \dots & \vert
\end{pmatrix}
+
\quad
\begin{pmatrix}
b^{[2]}\\
\vdots\\
b^{[2]}
\end{pmatrix}
$$


Generally the weights matrix of a given layer $l$ will have dimensions


$$
\text{dim}(\mathbf{W}^{[l]}) = (n_{l-1}, n_l)
$$

and the outputs from a given layer (for a single training example) are $1 \times n_{l}$ or $m \times n_{l}$ when considering all $m$ training examples.

### Non-linear step and activation


One again, we would apply an activation function to the outputs from the second hidden layer.

$$
\mathbf{A^{[2]}} = \sigma(\mathbf{Z}^{[2]})
$$

where $\mathbf{A}^{[2]}$ will also be a $m \times n_2$ dimensional matrix.

We'd continue by feeding this into the next hidden layer, and therefore would need $\mathbf{W}^{[3]}$
with dimensions $n_2 \times n_3$.
