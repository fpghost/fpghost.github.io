---
title: "Backprop"
categories:
  - Blog
tags:
  - ML
toc: true
toc_sticky: true
---

# Design matrix

Let's start with the design matrix $\mathbf{X}$ definition

$$
\mathbf{X}=\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
$$

This is a $m \times k$ matrix when we have $k$ features and $m$ training examples. For example, maybe we have $k=3$ features such as a student's GPA, GRE and class rank and we have $m=1000$ students in our sample, then we'd have a $1000 \times 3$ design matrix. Each row of that matrix is a vector representing one of our training examples, and the superscript here denotes which training example we are considering.

# Forward prop


## First hidden layer

<img src="/assets/images/nn.png" alt="Simple NN" class="full">


Consider if we had the next layer with just 2 nodes then the linear transformation would look like

### Linear step

$$
\mathbf{Z}^{[1]} = \mathbf{X}\mathbf{W}^{[1]}+\mathbf{b}^{[1]}
$$

where square-bracket superscripts denote the layer under consideration

$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
\begin{pmatrix}
    \vert & \vert \\
    w_1^{[1]}   & w_2^{[1]}   \\
    \vert & \vert
\end{pmatrix}
+
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{b}^{[1]} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{b}^{[1]} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
$$

so we had $\mathbf{X}$ being an $m \times k$ matrix, and $\mathbf{W}^{[1]}$ - the matrix of weights for the first layer - is a $k \times n_1$ dimensional matrix if we have $n_1$ nodes in this hidden layer. Here $n_1=2$. If have $m$ training examples we need $m \times n_1$ bias units too. In Python these are in practice "broadcast".


Imagine the above with a single training example or $m=1$

$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[1](1)} & \hspace{-0.2cm} \text{---} 
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{x}^{(1)} & \hspace{-0.2cm} \text{---} 
\end{pmatrix}
\begin{pmatrix}
    \vert & \vert \\
    w_1^{[1]}   & w_2^{[1]}   \\
    \vert & \vert
\end{pmatrix}
+
\quad
\begin{pmatrix}
    b^{[1]}_1 & b^{[1]}_2 
\end{pmatrix}

$$


We can see that 

$$\mathbf{z}^{[1](1)}$$

 is just a row-vector of shape $1\times n_1$, with each element representing the (linear) output of the neurons in the 1st hidden layer when applied to the first training example.

E.g. 

$$\mathbf{Z}^{[1]}_{11}$$

would mean the output from the first node, in the first hidden layer after being fed the first training example. 

and 

$$\mathbf{Z}^{[1]}_{12}$$

would be the output from the second node in the first hidden layer, after being fed the first training example. 

Going even further, if we reduced the nodes in this layer to just $1$, then we'd have the Perceptron,
and the output for a single training example would just be a scalar.

Note: $\mathbf{Z}=\mathbf{X}\mathbf{W}$ could also be written as $\mathbf{Z}^T=\mathbf{W}^T\mathbf{X}^T$
and some people may prefer that convention.




### Non-linear step and activation


Next, we would apply an activation function, such as the sigmoid, tanh or ReLu to the outputs from the first hidden layer.

$$
\mathbf{A^{[1]}} = \sigma(\mathbf{Z}^{[1]})
$$

where $\mathbf{A}^{[1]}$ will also be a $m \times n_1$ dimensional matrix.


## Subsequent hidden layers

All that happens now is a repeat, instead of feeding the next layer with the design matrix, $\mathbf{X}$, we now feed it $\mathbf{A}^{[1]}$


### Linear step


$$
\mathbf{Z}^{[2]} = \mathbf{A^{[1]}}\mathbf{W}^{[2]}+\mathbf{b}^{[2]}
$$

Remember $\mathbf{A}^{[1]}$ is a $m \times n_1$ dimensional matrix, where $m$ is the number of training examples we have and $n_1$ is how many nodes are in the first hidden layer.  

This is a bit like we now have $m$ training examples with $n_1$ features each. Each node of the 2nd hidden layer, needs to accept input from each of those $n_1$ features (or outputs from the previous layer if you prefer), meaning each node of the 2nd layer will need $n_1$ weights.

If we have $n_2$ nodes in the 2nd hidden layer, then $\mathbf{W}^{[2]}$ needs to be a $n_1 \times n_2$ dimensional matrix


$$
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[2](1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{z}^{[2](m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
=
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{a}^{[1](1)} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{a}^{[1](m)} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
\begin{pmatrix}
    \vert & \dots & \vert \\
    w_1^{[2]} & \dots  & w_{n_2}^{[2]}   \\
    \vert & \dots & \vert
\end{pmatrix}
+
\quad
\begin{pmatrix}
    \text{---} \hspace{-0.2cm} & \mathbf{b}^{[2]} & \hspace{-0.2cm} \text{---} \\
    \vdots \hspace{-0.2cm} & \vdots & \hspace{-0.2cm} \vdots \\ 
    \text{---} \hspace{-0.2cm} & \mathbf{b}^{[2]} & \hspace{-0.2cm} \text{---}
\end{pmatrix}
$$


Generally the weights matrix of a given layer $l$ will have dimensions


$$
\text{dim}(\mathbf{W}^{[l]}) = (n_{l-1}, n_l)
$$

and the outputs from a given layer (for a single training example) are $1 \times n_{l}$ or $m \times n_{l}$ when considering all $m$ training examples.

### Non-linear step and activation


Once again, we would apply an activation function to the outputs from the second hidden layer.

$$
\mathbf{A^{[2]}} = \sigma(\mathbf{Z}^{[2]})
$$

where $\mathbf{A}^{[2]}$ will also be a $m \times n_2$ dimensional matrix.

We'd continue by feeding this into the next hidden layer, and therefore would need $\mathbf{W}^{[3]}$
with dimensions $n_2 \times n_3$.


## Generic forward prop

$$
\mathbf{Z}^{[l+1]} = \mathbf{A^{[l]}}\mathbf{W}^{[l+1]}+\mathbf{b}^{[l+1]}
$$

Which, in terms of elements looks like


$$
\mathbf{Z}^{[l+1](i)}_{\quad \quad j} = \sum_{k=1}^{n_l} \mathbf{A}^{[l](i)}_{\quad\, k}\mathbf{W}^{[l+1]}_{kj}+\mathbf{b}^{[l+1](i)}_{\quad\quad\, j}
$$

Here the column-index $i$ refers to which training example is under consideration and the row index $j$ refers to which node in the layer. 

For example

$$
\mathbf{Z}^{[3](4)}_{2}
$$

means the linear output from the 2nd node of the 3rd layer of the NN when it was fed from the 4th training example.


# Cost function

Let's say we have some cost function that we consider as a function of the weights

$$
\mathcal{J}(\mathbf{W}) = \frac{1}{m} \sum_{i}^m \mathcal{L^{(i)}}(\mathbf{W}) 
$$

and $\mathcal{L_i}$ is the contribution made to that overall cost $\mathcal{J}$ by the ith training example.


We are going to be interested in the derivative of $\mathcal{J}$ with respect to all of our weights and biases (across all layers in the NN). In other words will want to compute


$$
\begin{aligned}
\frac{\partial \mathcal{J}}{\partial W^{[l]}_{jk}}&=\frac{1}{m}\frac{\partial}{\partial W^{[l]}_{jk}}\left( \sum_{i}^m \mathcal{L^{(i)}}(\mathbf{W}) \right)\\
&=\frac{1}{m} \sum_{i}^m \frac{\partial \mathcal{L^{(i)}}(\mathbf{W}) }{\partial W^{[l]}_{jk}}\quad \text{by linearity of the derivative}
\end{aligned}
$$

for all layers $l$ in the NN, and for all $j, k$ defined by the $1 < j < n_{l-1}$ and $1 < k <, n_{l}$ for a given layer $l$.

We can focus on computing the terms in this sum 

$$
dW_{jk}^{[l](i)} \stackrel{\text{def}}{=}\frac{\partial \mathcal{L^{(i)}}(\mathbf{W}) }{\partial W^{[l]}_{jk}}
$$

and then later combine them by summing over the training examples and the $i$ index.

We will also make another definition to help simplify later


$$
dZ_{\quad \, p}^{[l](i)} \stackrel{\text{def}}{=}\frac{\partial \mathcal{L^{(i)}}}{\partial Z^{[l](i)}_{\quad \,p}}
$$

## Chain rule


$$
\begin{aligned}
dW_{jk}^{[l](i)} &\stackrel{\text{def}}{=}\frac{\partial \mathcal{L^{(i)}}(\mathbf{W}) }{\partial W^{[l]}_{jk}}\\
&=\sum_{p=1}^{n_1} \frac{\partial \mathcal{L}^{(i)}}{\partial Z_{\quad \, p}^{[l](i)}}\frac{\partial Z_{\quad \, p}^{[l](i)}}{\partial W^{[l]}_{jk}}\\
&=\sum_{p=1}^{n_1} dZ_{\quad \,p}^{[l](i)} \frac{\partial Z_{\quad \, p}^{[l](i)}}{\partial W^{[l]}_{jk}}\\
&=\sum_{p=1}^{n_1} dZ_{\quad \,p}^{[l](i)} \frac{\partial }{\partial W^{[l]}_{jk}}\left[\sum_{q=1}^{n_{l-1}}A^{[l-1](i)}_{\quad\quad\, q}\mathbf{W}^{[l]}_{qp}+\mathbf{b}^{[l](i)}_{\quad\, p}
\right]\\
&=\sum_{p=1}^{n_1} dZ_{\quad \,p}^{[l](i)} \left[\sum_{q=1}^{n_{l-1}}A^{[l-1](i)}_{\quad\quad\, q}\frac{\partial W^{[l]}_{qp}}{\partial W^{[l]}_{jk}} \right]\quad \text{linearity of deriv}\\
&=\sum_{p=1}^{n_1} dZ_{\quad \,p}^{[l](i)} \left[\sum_{q=1}^{n_{l-1}}A^{[l-1](i)}_{\quad\quad\, q}\times \delta_{qj}\times \delta_{pk}\right]\\
&= dZ_{\quad \,k}^{[l](i)} A^{[l-1](i)}_{\quad\quad\, j}
\end{aligned}
$$


where here the [Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta)

$$
 \delta_{ij} =  \begin{cases}
   1, & \text{if } i=j\\
    0,    & \text{otherwise}
\end{cases}
$$

and just represents the facts that the derivative of the weights matrix elements are unity if the same element and zero if another element. This allowed us to collapse the sums nicely.


{% capture notice-1 %}
$$
dW_{jk}^{[l](i)} = dZ_{\quad \,k}^{[l](i)} A^{[l-1](i)}_{\quad\quad\, j}
$$
 
{% endcapture %}
<div class="notice">{{ notice-1 | markdownify }}</div>

If we now come back to our actual objective, the derivative of the cost summed over all training examples


$$
\begin{aligned}
\frac{\partial \mathcal{J}}{\partial W^{[l]}_{jk}}&
=\frac{1}{m} \sum_{i}^m dW_{jk}^{[l](i)}\\
&=\frac{1}{m} \sum_{i}^m  dZ_{\quad \,k}^{[l](i)} A^{[l-1](i)}_{\quad\quad\, j}\\
&=\frac{1}{m} \sum_{i}^m  (A^T)^{[l-1](j)}_{\quad\quad\, i} dZ_{\quad \,\,k}^{[l](i)}  \quad \text{taking transpose}\\
&=\frac{1}{m} \left((A^T)^{[l-1]} dZ^{[l]}\right)_{jk}\\
\end{aligned}
$$

and finally


$$
\begin{aligned}
\frac{\partial \mathcal{J}}{\partial \mathbf{W}^{[l]}}&
=\frac{1}{m} (\mathbf{A}^T)^{[l-1]} \mathbf{dZ}^{[l]}\\
&=\frac{1}{m} (\mathbf{A}^T)^{[l-1]} \frac{\partial \mathcal{L^{(i)}}}{\partial \mathbf{Z}^{[l]}}
\end{aligned}
$$


Recall that $\mathbf{W^{[l]}}$ has dimensions $(n_{l-1}, n_l)$, $\mathbf{A}^{[l-1]}$ has dimensions $(m, n_{l-1})$ and so $\mathbf{A}^T$ has dimensions $(n_{l-1}, m)$, and $\mathbf{Z}^{[l]}$ has dimensions of $(m, n_l)$, so the right hand side has dimensions $(m \times n_{l-1}) \times (m \times n_l)=n_{l-1} \times n_l$, matching what we expected for $\mathbf{W}^{[l]}$.


If I'd have used transposed definitions then I would have ended up with

$$
\begin{aligned}
\frac{\partial \mathcal{J}}{\partial (\mathbf{W}^T)^{[l]}}&
=\frac{1}{m} (\mathbf{dZ}^T)^{[l]} \mathbf{A}^{[l-1]}\\
\end{aligned}
$$

where $\mathbf{A}$ would be a $m \times n_{l-1}$ matrix with rows representing training examples and each column being the output from the given node in the layer, and $\mathbf{Z}^T$ would be a $n_l \times m$ matrix with columns representing training examples.